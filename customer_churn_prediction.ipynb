{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 18858,
          "sourceType": "datasetVersion",
          "datasetId": 13996
        },
        {
          "sourceId": 316432,
          "sourceType": "datasetVersion",
          "datasetId": 132995
        },
        {
          "sourceId": 2451163,
          "sourceType": "datasetVersion",
          "datasetId": 1372879
        }
      ],
      "dockerImageVersionId": 30396,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "# https://unsplash.com/photos/iUfusOthmgQ"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:43.013724Z",
          "iopub.execute_input": "2023-02-20T07:02:43.015027Z",
          "iopub.status.idle": "2023-02-20T07:02:43.04888Z",
          "shell.execute_reply.started": "2023-02-20T07:02:43.014861Z",
          "shell.execute_reply": "2023-02-20T07:02:43.047976Z"
        },
        "trusted": true,
        "id": "4KM7-Ty3zfL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:43.0509Z",
          "iopub.execute_input": "2023-02-20T07:02:43.051484Z",
          "iopub.status.idle": "2023-02-20T07:02:43.58767Z",
          "shell.execute_reply.started": "2023-02-20T07:02:43.051451Z",
          "shell.execute_reply": "2023-02-20T07:02:43.586773Z"
        },
        "trusted": true,
        "id": "OhzVX7RYzfL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"toc\"></a>\n",
        "- [1. Introduction](#1)\n",
        "    - [1.1 Important questions](#1.1)\n",
        "    - [1.2 Feature scaling Vs different machine learning algorithms](#1.2)\n",
        "    - [1.3 Normalization/standardization before or after train-test split?](#1.3)\n",
        "    - [1.4 Is it necessary to scale the target value?](#1.4)\n",
        "    - [1.5 Should we delete outliers before scaling?](#1.5)\n",
        "    - [1.6 Best practices](#1.6)\n",
        "- [2. Set-up](#2)\n",
        "    - [2.1 Import Libraries](#2.1)\n",
        "    - [2.2 Import Data](#2.2)\n",
        "    - [2.3 Data Set Characteristics](#2.3)\n",
        "- [3. Data preprocessing](#3)\n",
        "    - [3.1 Dealin with missing values in TotalCharges](#3.1)\n",
        "    - [3.2 Dealing with duplicate values](#3.2)\n",
        "    - [3.3 Creating numerical and categorical lists](#3.3)\n",
        "    - [3.4 Feature scaling](#3.4)\n",
        "- [4. Different scalers and common misconceptions](#4)\n",
        "    - [4.1 MinMaxScaler normalization](#4.1)\n",
        "        - [4.1.1 MinMax Scaler - when to use?](#4.1.1)\n",
        "    - [4.2 StandardScaler standardization](#4.2)\n",
        "    - [4.3 Is it valid to standardize variables with non-normal distribution?](#4.3)\n",
        "    - [4.4 RobustScaler standardization](#4.4)\n",
        "- [5. Outliers (and highly skewed data)](#5)\n",
        "- [6. One more example (and comparison)](#6)\n",
        "    - [6.1 Is it always better to use the RobustScaler?](#6.1)\n",
        "- [6. References](#6)"
      ],
      "metadata": {
        "id": "bYr9m1pwzfL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Introduction - The Scale of Your Data Matters**"
      ],
      "metadata": {
        "id": "hgB6tkiBzfL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The more I’m involved in machine learning, the more I'm convinced that a deep understanding of what we are doing during data preprocessing highly affects the outcome.\n",
        "\n",
        "Data transformation is one of the fundamental steps in the part of data processing. The primary reason we standardize or normalize variables is to make the features comparable by bringing them to the same scale. Very often our dataset contains features highly varying in magnitudes, units and range. Using the original scale may put more weights on the variables with a large range. This is highly undesirable.\n",
        "\n",
        "Desirable outcome:\n",
        "1. numerical features are scaled to a standard range;\n",
        "2. the scaling process allows the algorithm to give equal weights to the variables."
      ],
      "metadata": {
        "id": "Y6lcNjbdzfL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"background-color:#CDA63A;\n",
        "                                                 color:white;\n",
        "                                                 border-color:black;\n",
        "                                                 border-radius:5px;\n",
        "                                                 width:50%;\n",
        "                                                 margin: auto;\n",
        "                                                 text-align: left;\">\n",
        "<b>In essence:</b>\n",
        "the terms normalization and standardization are sometimes used interchangeably, but they usually refer to different things.\n",
        "<li>the result of standardization (or Z-score normalization) is that the features will be rescaled to ensure the mean and the standard deviation to be 0 and 1, respectively. Standardization is not bounded by range;\n",
        "<li>the result of normalization (using MinMaxScaler) is always in a range of [0,1] or [-1,1] if there are negative values.\n",
        "</div>"
      ],
      "metadata": {
        "id": "yAUdFwIvzfL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"1.1\"></a>\n",
        "## <b>1.1 <span style='color:#E1B12D'>Important questions</span></b>"
      ],
      "metadata": {
        "id": "JkLjwC4UzfL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will try to answer all important questions related to the process of scaling variables like:\n",
        "1. Does StandardScaler really assume Gaussian distribution and what are the consequences?\n",
        "2. Is it valid to standardize variables with non-normal distribution?\n",
        "3. What are the drawbacks and common misconceptions of standardizing and normalizing process?\n",
        "4. Which technique is better? Should I normalize or standardize?\n",
        "5. Is RobustScaler really robust and always the best choice?"
      ],
      "metadata": {
        "id": "US684adQzfL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"1.2\"></a>\n",
        "## <b>1.2 <span style='color:#E1B12D'>Feature scaling Vs different machine learning algorithms</span></b>"
      ],
      "metadata": {
        "id": "Jpg6hO9jzfL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The difference in scale for input variables does not affect all machine learning algorithms. Some of them are sensitive to feature scaling while others are not.\n",
        "\n",
        "* **Gradient Descent Algorithm**\n",
        "\n",
        "Machine learning algorithms like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled.\n",
        "\n",
        "* **Distance Based-Algorithm**\n",
        "\n",
        "Distance algorithms like KNN, K-means, and SVM are most affected by the range of features. This is because they are using distances between data points to determine their similarity.\n",
        "\n",
        "* **Tree-based algorithms** are more robust to the scale of the features.\n",
        "\n",
        "Scaling is critical, while performing **Principal Component Analysis(PCA)**. PCA tries to get the features with maximum variance and the variance is high for high magnitude features. This skews the PCA towards high magnitude features.\n",
        "\n"
      ],
      "metadata": {
        "id": "hL9s0Mu6zfL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"1.3\"></a>\n",
        "## <b>1.3 <span style='color:#E1B12D'>Normalization/standardization before or after train-test split?</span></b>"
      ],
      "metadata": {
        "id": "tbO_YN2pzfL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should never forget that the testing data set represents real-world data. So normalize or standardize always AFTER splitting data. If you standardize (take the mean and variance of the whole dataset) before splitting, you'll introduce future information into the training explanatory variables (i.e. the mean and variance)."
      ],
      "metadata": {
        "id": "qT8FJfpgzfL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"1.4\"></a>\n",
        "## <b>1.4 <span style='color:#E1B12D'>Is it necessary to scale the target value?</span></b>"
      ],
      "metadata": {
        "id": "M5XQ1_9izfL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling the target value is a good idea in regression modelling. Scaling of the data makes it easy for a model to learn and understand the problem.\n",
        "\n",
        "\"A target variable with a large spread of values, in turn, may result in large error gradient values causing weight values to change dramatically, making the learning process unstable.\"\n",
        "\n",
        "[Neural Networks for Pattern Recognition (Advanced Texts in Econometrics](https://www.amazon.com/Networks-Recognition-Advanced-Econometrics-Paperback/dp/0198538642/ref=as_li_ss_tl?ie=UTF8&qid=1540160671&sr=8-2&keywords=Neural+Networks+for+Pattern+Recognition&linkCode=sl1&tag=inspiredalgor-20&linkId=991aca4ff0fc6769d5dad40a86092458&language=en_US)\n",
        "\n",
        "— Page 298, Neural Networks for Pattern Recognition, 1995."
      ],
      "metadata": {
        "id": "P_a_Qk8NzfL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"1.5\"></a>\n",
        "## <b>1.5 <span style='color:#E1B12D'>Should we delete outliers before scaling?</span></b>"
      ],
      "metadata": {
        "id": "k8hNV5bJzfL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Including outliers in data driven models could be risky. The existence of an extreme single misleading value has the potential to change the conclusion implied by the model. It is therefore important to manage that kind of risk.\n",
        "\n",
        "**So delete or not delete?**\n",
        "\n",
        "Well it depends. Especially when a data set is highly imbalanced, deleting outliers could possibly be very undesirable, because by doing that we can drop important data.\n",
        "\n",
        "After dropping outliers we should always check what we have done. Sometimes we should definitely keep the outliers in a dataset.\n",
        "\n",
        "Take a look at this notebook:\n",
        "* [How to create a meaningful EDA](https://www.kaggle.com/code/marcinrutecki/how-to-create-a-meaningful-eda)\n",
        "\n",
        "By deleting outliers we dropped about 40% of the very important data in that case! We shouldn't do that!"
      ],
      "metadata": {
        "id": "rbE8i_NrzfL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the data is balanced we should consider deleting outliers.\n",
        "\n",
        "> Check this notebook for more information when dealing with Outliers: [Outlier detection methods!](https://www.kaggle.com/code/marcinrutecki/outlier-detection-methods)"
      ],
      "metadata": {
        "id": "wrD9fa1szfL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"1.6\"></a>\n",
        "## <b>1.6 <span style='color:#E1B12D'>Best practices</span></b>"
      ],
      "metadata": {
        "id": "KGpS_OG2zfL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\" background-color:#3b3745;\n",
        "            text-align:left;\n",
        "            padding: 13px 13px;\n",
        "            border-radius: 8px;\n",
        "            margin: auto;\n",
        "            color: white\">\n",
        "<ul>\n",
        "<li> It’s usually a bad idea to train machine learning models on features with vastly different scales.\n",
        "<li> Consider deleting outliers before scaling a data, but be very careful when a data is highly imbalanced.\n",
        "<li> Some machine learning algorithms are sensitive to feature scaling (gradient descent, distance based) while others are not (tree-based).\n",
        "<li> Normalize or standardize always AFTER splitting data (of course you need to apply normalization/standardization to test data as well).\n",
        "<li> StandardScaler, MinMaxScaler and RobustScaler will not affect data distribution (the algorithms don't enforce a normal distribution).\n",
        "\n",
        "<li> Normalisation does not treat outliers well (MinMax Scaler shrinks the data within the given range which will affect the ability of the algorithm to give adequate weights for the features).\n",
        "<li> MinMaxScaler can be used when the upper and lower boundaries are well known from domain knowledge.\n",
        "\n",
        "<li> Standardization allows better handling of the outliers and better facilitates convergence of features (which is important for the gradient descent and distance based algorithms).\n",
        "<li> StandardScaler is also sensitive to outliers (less than MinMaxScaler, but still sensitive). The mean and standard deviation are highly affected by outliers so the distance (measured in standard deviation) is also affected by outliers - the algorithm will scale most of the data to a small interval.\n",
        "<li> RobustScaler's results are not skewed by outliers and the spread represents REAL distance.\n",
        "<li> RobustScaler's deliberate exclusion of certain data can limit the overall amount of information that is used to make predictions, leading to lower confidence of the result.\n",
        "<li> In my opinion there are no possible issues with standardizing non-normal distribution.\n",
        "\n",
        "<li> You can always start by fitting your model to raw, normalized and standardized data and compare the performance for best results.\n",
        "\n",
        "***   \n",
        "\n",
        "<b>I usually prefer standardization over normalization.\n",
        "    \n",
        "I would take advantage of StandardScaler if:\n",
        "\n",
        "* I decide to delete outliers or they are not present in a data set;\n",
        "* when the data is not very highly skewed.\n",
        "\n",
        "I would use RobustScaler in opposite situations and MinMaxScaler only in a very special cases.\n",
        "\n",
        "</ul>\n",
        "</div>"
      ],
      "metadata": {
        "id": "zxa7agtCzfL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >🔝Back to Table of Contents🔝</a>"
      ],
      "metadata": {
        "id": "yowwdl6wzfL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"2.1\"></a>\n",
        "## <b>2.1 <span style='color:#E1B12D'>Import Libraries</span></b>"
      ],
      "metadata": {
        "id": "Hkb32IcrzfL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import tkinter\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from collections import Counter"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:43.589021Z",
          "iopub.execute_input": "2023-02-20T07:02:43.589379Z",
          "iopub.status.idle": "2023-02-20T07:02:44.908532Z",
          "shell.execute_reply.started": "2023-02-20T07:02:43.589344Z",
          "shell.execute_reply": "2023-02-20T07:02:44.907315Z"
        },
        "trusted": true,
        "id": "8XZDW9ZjzfL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"2.2\"></a>\n",
        "## <b>2.2 <span style='color:#E1B12D'>Import Data</span></b>"
      ],
      "metadata": {
        "id": "2MIwanOyzfL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df = pd.read_csv('telco.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:44.910158Z",
          "iopub.execute_input": "2023-02-20T07:02:44.910603Z",
          "iopub.status.idle": "2023-02-20T07:02:44.977733Z",
          "shell.execute_reply.started": "2023-02-20T07:02:44.910561Z",
          "shell.execute_reply": "2023-02-20T07:02:44.976585Z"
        },
        "trusted": true,
        "id": "EyMmjXKyzfL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"2.3\"></a>\n",
        "## <b>2.3 <span style='color:#E1B12D'>Data Set Characteristics</span></b>"
      ],
      "metadata": {
        "id": "O4MVyjFyzfL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each row represents a customer, each column contains customer’s attributes described on the column Metadata.\n",
        "\n",
        "The data set includes information about:\n",
        "\n",
        "* Customers who left within the last month – the column is called Churn.\n",
        "* Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies.\n",
        "* Customer account information – how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges.\n",
        "* Demographic info about customers – gender, age range, and if they have partners and dependents.\n",
        "\n",
        "The are no missing values in data set."
      ],
      "metadata": {
        "id": "VTUr3xVPzfL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "palette = ['#008080','#FF6347', '#E50000', '#D2691E'] # Creating color palette for plots"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:44.980873Z",
          "iopub.execute_input": "2023-02-20T07:02:44.981406Z",
          "iopub.status.idle": "2023-02-20T07:02:44.987001Z",
          "shell.execute_reply.started": "2023-02-20T07:02:44.98137Z",
          "shell.execute_reply": "2023-02-20T07:02:44.985445Z"
        },
        "trusted": true,
        "id": "PxWI_ukCzfL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >🔝Back to Table of Contents🔝</a>"
      ],
      "metadata": {
        "id": "zZIyPSeQzfL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. **Data preprocessing**"
      ],
      "metadata": {
        "id": "C-43GShEzfL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't need a customerID column, so I'll delete it."
      ],
      "metadata": {
        "id": "2DwGICZZzfL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = raw_df.drop('customerID', axis=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:44.98894Z",
          "iopub.execute_input": "2023-02-20T07:02:44.989404Z",
          "iopub.status.idle": "2023-02-20T07:02:45.011219Z",
          "shell.execute_reply.started": "2023-02-20T07:02:44.98936Z",
          "shell.execute_reply": "2023-02-20T07:02:45.009363Z"
        },
        "trusted": true,
        "id": "bmSLYogFzfL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dealing with missing values in TotalCharges"
      ],
      "metadata": {
        "id": "kr3kplwGzfL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # df['TotalCharges'] = df['TotalCharges'].astype(float)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:45.013816Z",
          "iopub.execute_input": "2023-02-20T07:02:45.014879Z",
          "iopub.status.idle": "2023-02-20T07:02:45.018915Z",
          "shell.execute_reply.started": "2023-02-20T07:02:45.014841Z",
          "shell.execute_reply": "2023-02-20T07:02:45.01805Z"
        },
        "trusted": true,
        "id": "xLIPY7MGzfL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An error occured when trying to execule a code above: could not convert string to float: ''\n",
        "\n",
        "The string to be converted must not contain any characters or symbols. The error occurs due to the incorrect initialization of value to a string variable.\n",
        "\n",
        "We have probably empty strings in 'TotalCharges' colums, but as they were defined as string, they didn't appear as Null Values."
      ],
      "metadata": {
        "id": "nbP_zDThzfL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step1 = [len(i.split()) for i in df['TotalCharges']] # splitting individual elements of 'Total_Charges'\n",
        "step2 = [i for i in range(len(step1)) if step1[i] != 1] # storing the index values of 'Total_Charges' where length is not equal to 1.\n",
        "print('Number of entries with empty string: ', len(step2))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:45.020578Z",
          "iopub.execute_input": "2023-02-20T07:02:45.021532Z",
          "iopub.status.idle": "2023-02-20T07:02:45.042225Z",
          "shell.execute_reply.started": "2023-02-20T07:02:45.021488Z",
          "shell.execute_reply": "2023-02-20T07:02:45.040723Z"
        },
        "trusted": true,
        "id": "LgcG0qQ2zfL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We coud try to impute the missing values by building a model for that reason or fill them using some often used values in that kind of situations e.g. mean, median or mode, but it's simpler (and probably better) to just drop the coresponding rows from a dataset as the numbers of empty string is very low (11)."
      ],
      "metadata": {
        "id": "qYpE4W7rzfL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(step2, axis = 0).reset_index(drop=True) # Dropping rows with no values in 'Total_charges'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:45.043674Z",
          "iopub.execute_input": "2023-02-20T07:02:45.044807Z",
          "iopub.status.idle": "2023-02-20T07:02:45.055515Z",
          "shell.execute_reply.started": "2023-02-20T07:02:45.044771Z",
          "shell.execute_reply": "2023-02-20T07:02:45.054422Z"
        },
        "trusted": true,
        "id": "8p6hdlBTzfL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['TotalCharges'] = df['TotalCharges'].astype(float) # Finally we can convert string to float in 'Total_charges' column"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:45.057104Z",
          "iopub.execute_input": "2023-02-20T07:02:45.057651Z",
          "iopub.status.idle": "2023-02-20T07:02:45.065583Z",
          "shell.execute_reply.started": "2023-02-20T07:02:45.057618Z",
          "shell.execute_reply": "2023-02-20T07:02:45.064505Z"
        },
        "trusted": true,
        "id": "GeJ6bHgfzfL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"3.2\"></a>\n",
        "## <b>3.2 <span style='color:#E1B12D'>Dealing with duplicated values</span></b>"
      ],
      "metadata": {
        "id": "MSPBbhNBzfL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of duplicated values in training dataset: ', df.duplicated().sum())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:45.067091Z",
          "iopub.execute_input": "2023-02-20T07:02:45.06773Z",
          "iopub.status.idle": "2023-02-20T07:02:45.098659Z",
          "shell.execute_reply.started": "2023-02-20T07:02:45.067696Z",
          "shell.execute_reply": "2023-02-20T07:02:45.097212Z"
        },
        "trusted": true,
        "id": "bP6eLoCezfL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(inplace=True)\n",
        "print(\"Duplicated values dropped succesfully\")\n",
        "print(\"*\" * 100)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:45.100027Z",
          "iopub.execute_input": "2023-02-20T07:02:45.100464Z",
          "iopub.status.idle": "2023-02-20T07:02:45.124373Z",
          "shell.execute_reply.started": "2023-02-20T07:02:45.100421Z",
          "shell.execute_reply": "2023-02-20T07:02:45.122996Z"
        },
        "trusted": true,
        "id": "tJ2SJWezzfL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"3.3\"></a>\n",
        "## <b>3.3 <span style='color:#E1B12D'>Creating numerical and categorical lists</span></b>"
      ],
      "metadata": {
        "id": "b93VjQX1zfL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# distinction is based on the number of different values in the column\n",
        "columns = list(df.columns)\n",
        "\n",
        "categoric_columns = []\n",
        "numeric_columns = []\n",
        "\n",
        "for i in columns:\n",
        "    if len(df[i].unique()) > 6:\n",
        "        numeric_columns.append(i)\n",
        "    else:\n",
        "        categoric_columns.append(i)\n",
        "\n",
        "categoric_columns = categoric_columns[:-1] # Excluding target:'Churn'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:45.125645Z",
          "iopub.execute_input": "2023-02-20T07:02:45.125991Z",
          "iopub.status.idle": "2023-02-20T07:02:45.149299Z",
          "shell.execute_reply.started": "2023-02-20T07:02:45.125959Z",
          "shell.execute_reply": "2023-02-20T07:02:45.147744Z"
        },
        "trusted": true,
        "id": "ZIT8HygGzfL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"3.4\"></a>\n",
        "## <b>3.4 <span style='color:#E1B12D'>Feature scaling</span></b>"
      ],
      "metadata": {
        "id": "UOS1kx8MzfL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "\n",
        "# Creating functions for scaling\n",
        "def Standard_Scaler (df, col_names):\n",
        "    features = df[col_names]\n",
        "    scaler = StandardScaler().fit(features.values)\n",
        "    features = scaler.transform(features.values)\n",
        "    df[col_names] = features\n",
        "    return df\n",
        "\n",
        "def MinMax_Scaler (df, col_names):\n",
        "    features = df[col_names]\n",
        "    scaler = MinMaxScaler().fit(features.values)\n",
        "    features = scaler.transform(features.values)\n",
        "    df[col_names] = features\n",
        "    return df\n",
        "\n",
        "def Robust_Scaler (df, col_names):\n",
        "    features = df[col_names]\n",
        "    scaler = RobustScaler().fit(features.values)\n",
        "    features = scaler.transform(features.values)\n",
        "    df[col_names] = features\n",
        "    return df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:45.155894Z",
          "iopub.execute_input": "2023-02-20T07:02:45.156575Z",
          "iopub.status.idle": "2023-02-20T07:02:45.165585Z",
          "shell.execute_reply.started": "2023-02-20T07:02:45.156532Z",
          "shell.execute_reply": "2023-02-20T07:02:45.164206Z"
        },
        "trusted": true,
        "id": "hYkVNsn7zfL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"background-color:#CDA63A;\n",
        "                                                 color:white;\n",
        "                                                 border-color:black;\n",
        "                                                 border-radius:5px;\n",
        "                                                 width:50%;\n",
        "                                                 margin: auto;\n",
        "                                                 text-align: left;\">\n",
        "It is important to remember that the fit method is used for calculating the mean and variance of each of the features present in our data set. The transform method is transforming all the features using the respective mean and variance. In our case (this notebook) we transform the entire data set without splitting, but when we perform train-test split we have to remember that fit and transform is for train and transform for test set.\n",
        "</div>"
      ],
      "metadata": {
        "id": "u3SkbQR4zfL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_names = numeric_columns\n",
        "df_ss = df.copy()\n",
        "df_ss = Standard_Scaler (df_ss, col_names)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:45.167137Z",
          "iopub.execute_input": "2023-02-20T07:02:45.167498Z",
          "iopub.status.idle": "2023-02-20T07:02:45.184126Z",
          "shell.execute_reply.started": "2023-02-20T07:02:45.167465Z",
          "shell.execute_reply": "2023-02-20T07:02:45.182793Z"
        },
        "trusted": true,
        "id": "_udQLIZfzfL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mm = df.copy()\n",
        "df_mm = MinMax_Scaler (df_mm, col_names)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:45.185509Z",
          "iopub.execute_input": "2023-02-20T07:02:45.18629Z",
          "iopub.status.idle": "2023-02-20T07:02:45.199212Z",
          "shell.execute_reply.started": "2023-02-20T07:02:45.18624Z",
          "shell.execute_reply": "2023-02-20T07:02:45.197862Z"
        },
        "trusted": true,
        "id": "A4lmQPcYzfL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rs = df.copy()\n",
        "df_rs = Robust_Scaler (df_rs, col_names)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:45.200784Z",
          "iopub.execute_input": "2023-02-20T07:02:45.201203Z",
          "iopub.status.idle": "2023-02-20T07:02:45.215507Z",
          "shell.execute_reply.started": "2023-02-20T07:02:45.201169Z",
          "shell.execute_reply": "2023-02-20T07:02:45.214414Z"
        },
        "trusted": true,
        "id": "_J2uBxNlzfL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >🔝Back to Table of Contents🔝</a>"
      ],
      "metadata": {
        "id": "UCka0CgkzfL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Different scalers and common misconceptions**"
      ],
      "metadata": {
        "id": "LIPY26WLzfL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"4.1\"></a>\n",
        "## <b>4.1 <span style='color:#E1B12D'> MinMaxScaler normalization</span></b>"
      ],
      "metadata": {
        "id": "nH8kC9ibzfL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\" background-color:#3b3745;\n",
        "            text-align:left;\n",
        "            padding: 13px 13px;\n",
        "            border-radius: 8px;\n",
        "            margin: auto;\n",
        "            color: white\">\n",
        "<ul>\n",
        "MinMaxScaler formula:\n",
        "\n",
        "$x' =  \\frac {x − min(x)}{max(x)-min(x)}$\n",
        "</div>"
      ],
      "metadata": {
        "id": "9JivLRD_zfL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This technique is used to re-scales features with a distribution value between 0 and 1. For every feature, the minimum value gets transformed into 0 and the maximum value gets transformed into 1.\n",
        "\n",
        "In many sources we can read that normalization is used when the data doesn't have Gaussian distribution whereas Standardization is used on data having Gaussian distribution. Personally, I am skeptical about this idea."
      ],
      "metadata": {
        "id": "kee96it-zfL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (20,7))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "ax = sns.scatterplot(data=df, x= \"TotalCharges\", y=\"tenure\",color='#008080', s=9)\n",
        "ax.set(title = \"Before scaling\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "ax = sns.scatterplot(data=df_mm, x= \"TotalCharges\", y=\"tenure\",color='#FF6347', s=9)\n",
        "ax.set(title = \"After scaling (MinMaxScaler)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:45.216635Z",
          "iopub.execute_input": "2023-02-20T07:02:45.218238Z",
          "iopub.status.idle": "2023-02-20T07:02:45.873373Z",
          "shell.execute_reply.started": "2023-02-20T07:02:45.218191Z",
          "shell.execute_reply": "2023-02-20T07:02:45.872392Z"
        },
        "trusted": true,
        "id": "xwWcPqQlzfL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (20,9))\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "ax = sns.kdeplot(df[\"tenure\"], color='#008080', fill= True, alpha=.7, linewidth=0)\n",
        "ax.set(title = \"Before scaling\")\n",
        "ax.set_xlabel('Tenure')\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "ax = sns.kdeplot(df_mm[\"tenure\"], color='#FF6347', fill= True, alpha=.7, linewidth=0)\n",
        "ax.set(title = \"After scaling (MinMaxScaler)\")\n",
        "ax.set_xlabel('Tenure')\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "ax = sns.kdeplot(df[\"TotalCharges\"], color='#008080', fill= True, alpha=.7, linewidth=0)\n",
        "ax.set_xlabel('Total Charges')\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "ax = sns.kdeplot(df_mm[\"TotalCharges\"], color='#FF6347', fill= True, alpha=.7, linewidth=0)\n",
        "ax.set_xlabel('Total Charges')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:45.874681Z",
          "iopub.execute_input": "2023-02-20T07:02:45.875514Z",
          "iopub.status.idle": "2023-02-20T07:02:46.98782Z",
          "shell.execute_reply.started": "2023-02-20T07:02:45.875478Z",
          "shell.execute_reply": "2023-02-20T07:02:46.986667Z"
        },
        "trusted": true,
        "id": "BtliAYM4zfL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plots we can understand that the distributions after MinMaxScaler remain intact, but the scales have changed (as expected)."
      ],
      "metadata": {
        "id": "-wpQSfE5zfL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"4.1.1\"></a>\n",
        "#### <b>4.1.1 <span style='color:#E1B12D'>MinMax Scaler - when to use?</span></b>"
      ],
      "metadata": {
        "id": "7fbpmbFfzfL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MinMaxScaler may be used when the upper and lower boundaries are well known from domain knowledge.  A popular application is image processing, where pixel intensities have to be normalized to fit within a certain range (i.e., 0 to 255 for the RGB colour range)."
      ],
      "metadata": {
        "id": "RUdbcDpyzfL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"4.2\"></a>\n",
        "## <b>4.2 <span style='color:#E1B12D'> StandardScaler standardization</span></b>"
      ],
      "metadata": {
        "id": "iT_BptFmzfL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\" background-color:#3b3745;\n",
        "            text-align:left;\n",
        "            padding: 13px 13px;\n",
        "            border-radius: 8px;\n",
        "            margin: auto;\n",
        "            color: white\">\n",
        "<ul>\n",
        "StandardScaler formula:\n",
        "\n",
        "$x' = \\frac {x-μ}{σ}$\n",
        "\n",
        "μ - mean\n",
        "\n",
        "σ - standard deviation\n",
        "</div>"
      ],
      "metadata": {
        "id": "IGtOuNt7zfL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In many places on the Internet you can read that StandardScaler assumes that the data follows a Gaussian distribution. We usually draw a conclusion from that statement that we shouldn't take advantage of StandardScaler when a data is not normally distributed. In my opinion this is not true. StandardScaler will provide the best results when data is normally distributed  - that is a fact, but the same statement is valid also for MinMax Scaler. And it doesn’t necessarily mean that we cannot standardize non normal data. Let's take a look at the charts."
      ],
      "metadata": {
        "id": "hDQcr3wazfL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (20,7))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "ax = sns.scatterplot(data=df, x= \"TotalCharges\", y=\"tenure\",color='#008080', s=9)\n",
        "ax.set(title = \"Before scaling\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "ax = sns.scatterplot(data=df_ss, x= \"TotalCharges\", y=\"tenure\",color='#FF6347', s=9)\n",
        "ax.set(title = \"After scaling (StandardScaler)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:46.989271Z",
          "iopub.execute_input": "2023-02-20T07:02:46.989673Z",
          "iopub.status.idle": "2023-02-20T07:02:47.737829Z",
          "shell.execute_reply.started": "2023-02-20T07:02:46.989638Z",
          "shell.execute_reply": "2023-02-20T07:02:47.736471Z"
        },
        "trusted": true,
        "id": "qnXXNkt1zfL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (20,9))\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "ax = sns.kdeplot(df[\"tenure\"], color='#008080', fill= True, alpha=.7, linewidth=0)\n",
        "ax.set(title = \"Before scaling\")\n",
        "ax.set_xlabel('Tenure')\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "ax = sns.kdeplot(df_ss[\"tenure\"], color='#FF6347', fill= True, alpha=.7, linewidth=0)\n",
        "ax.set(title = \"After scaling (StandardScaler)\")\n",
        "ax.set_xlabel('Tenure')\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "ax = sns.kdeplot(df[\"TotalCharges\"], color='#008080', fill= True, alpha=.7, linewidth=0)\n",
        "ax.set_xlabel('Total Charges')\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "ax = sns.kdeplot(df_ss[\"TotalCharges\"], color='#FF6347', fill= True, alpha=.7, linewidth=0)\n",
        "ax.set_xlabel('Total Charges')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:47.739567Z",
          "iopub.execute_input": "2023-02-20T07:02:47.740064Z",
          "iopub.status.idle": "2023-02-20T07:02:48.657147Z",
          "shell.execute_reply.started": "2023-02-20T07:02:47.74002Z",
          "shell.execute_reply": "2023-02-20T07:02:48.655977Z"
        },
        "trusted": true,
        "id": "MbqO6cTozfL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above scatter plots and KDE plots we can see that the data distributions remain the same after applying StandardScaler. Only the scale changes. No matter if the data is normally distributed or not."
      ],
      "metadata": {
        "id": "OqWzfu7azfL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"4.3\"></a>\n",
        "## <b>4.3 <span style='color:#E1B12D'> Is it valid to standardize variables with non-normal distribution?</span></b>"
      ],
      "metadata": {
        "id": "WDbWqyLzzfL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again it is worth highlighting: StandardScaler will not affect data distribution. If you thought it was transforming the distribution of the data, you need to get out of that concept. It's important to keep in mind that StandardScaler only standardizes the features in the data, it does not enforce a normal distribution.\n",
        "\n",
        "<div style=\" background-color:#3b3745;\n",
        "            text-align:left;\n",
        "            padding: 13px 13px;\n",
        "            border-radius: 8px;\n",
        "            margin: auto;\n",
        "            color: white\">\n",
        "<ul>\n",
        "<li>In my opinion there are no possible issues with standardizing non-normal distribution. In other words, if the features do not follow a normal distribution before standardization, they will not follow it after the standardization either (the distribution will remain exactly the same - only the scale will change). So, it’s valid to do it on a non-normal distribution.\n",
        "    \n",
        "<li>Standardization also does not assume anything about the distribution of variables (normal or other).\n",
        "\n",
        "<li>Go ahead and standardize the variables (if required) without worrying about the distribution!\n",
        "<li>But be careful when a data is highly skewed or have significant outliers (topic discussed later)!\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "tWFydSm7zfL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"4.4\"></a>\n",
        "## <b>4.4 <span style='color:#E1B12D'>RobustScaler standardization</span></b>"
      ],
      "metadata": {
        "id": "cCHBJ4DxzfL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\" background-color:#3b3745;\n",
        "            text-align:left;\n",
        "            padding: 13px 13px;\n",
        "            border-radius: 8px;\n",
        "            margin: auto;\n",
        "            color: white\">\n",
        "<ul>\n",
        "RobustScaler formula:\n",
        "\n",
        "$x' =  \\frac {x − median(x)}{IQR}$\n",
        "\n",
        "IQR - describes the middle 50% of values when ordered from lowest to highest (IQR = Q3 – Q1).\n",
        "\n",
        "The scaled values will have their median and IQR set to 0 and 1, respectively.\n",
        "</div>"
      ],
      "metadata": {
        "id": "oCJLgu_IzfL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Interquartile.png](attachment:d1d0c176-f2b7-4025-a138-9d1698c48fef.png)"
      ],
      "metadata": {
        "id": "epxMu1F0zfL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different approach to standardizing input variables in the presence of outliers is to ignore them from the calculation of the mean and standard deviation, then use the calculated values to scale the variable.\n",
        "\n",
        "Unlike the previous scalers, the centering and scaling statistics of RobustScaler are based on percentiles and are therefore not influenced by a small number of very large marginal outliers. Consequently, the resulting range of the transformed feature values is larger than for the previous scalers and, more importantly, are approximately similar.\n",
        "\n",
        "The resulting variable is not skewed by outliers and the outliers are still present with the same relative relationships to other values.\n",
        "\n",
        "<div style=\" background-color:#3b3745;\n",
        "            text-align:left;\n",
        "            padding: 13px 13px;\n",
        "            border-radius: 8px;\n",
        "            margin: auto;\n",
        "            color: white\">\n",
        "<ul>\n",
        "<li> StandardScaler uses mean and standard deviation. RobustScaler uses median and interquartile range (IQR) instead.\n",
        "<li> Outliers can significantly alter the mean, but don't affect the median. That’s because the median doesn’t depend on every value in the list.\n",
        "</div>"
      ],
      "metadata": {
        "id": "6qSOFTqzzfL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (20,7))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "ax = sns.scatterplot(data=df, x= \"TotalCharges\", y=\"tenure\",color='#008080', s=9)\n",
        "ax.set(title = \"Before scaling\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "ax = sns.scatterplot(data=df_rs, x= \"TotalCharges\", y=\"tenure\",color='#FF6347', s=9)\n",
        "ax.set(title = \"After scaling (RobustScaler)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:48.658792Z",
          "iopub.execute_input": "2023-02-20T07:02:48.659458Z",
          "iopub.status.idle": "2023-02-20T07:02:49.129717Z",
          "shell.execute_reply.started": "2023-02-20T07:02:48.659416Z",
          "shell.execute_reply": "2023-02-20T07:02:49.128172Z"
        },
        "trusted": true,
        "id": "cBrBd1ZFzfL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (20,9))\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "ax = sns.kdeplot(df[\"tenure\"], color='#008080', fill= True, alpha=.7, linewidth=0)\n",
        "ax.set(title = \"Before scaling\")\n",
        "ax.set_xlabel('Tenure')\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "ax = sns.kdeplot(df_rs[\"tenure\"], color='#FF6347', fill= True, alpha=.7, linewidth=0)\n",
        "ax.set(title = \"After scaling (RobustScaler)\")\n",
        "ax.set_xlabel('Tenure')\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "ax = sns.kdeplot(df[\"TotalCharges\"], color='#008080', fill= True, alpha=.7, linewidth=0)\n",
        "ax.set_xlabel('Total Charges')\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "ax = sns.kdeplot(df_rs[\"TotalCharges\"], color='#FF6347', fill= True, alpha=.7, linewidth=0)\n",
        "ax.set_xlabel('Total Charges')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:49.131595Z",
          "iopub.execute_input": "2023-02-20T07:02:49.132017Z",
          "iopub.status.idle": "2023-02-20T07:02:50.098669Z",
          "shell.execute_reply.started": "2023-02-20T07:02:49.131976Z",
          "shell.execute_reply": "2023-02-20T07:02:50.097442Z"
        },
        "trusted": true,
        "id": "sTMJThuBzfL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again - the distributions after RobustScaler remain intact and outliers ARE still present!"
      ],
      "metadata": {
        "id": "YPoeXOjFzfL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >🔝Back to Table of Contents🔝</a>"
      ],
      "metadata": {
        "id": "Wf8r246azfL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Outliers (and highly skewed data)**"
      ],
      "metadata": {
        "id": "rqSje4ZOzfL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Min-max normalization has a significant downside: it does not handle skewed data and outliers very well. For example, if you have 99 values between 0 and 40, and one value is 100, then the 99 values will all be transformed to a value between 0 and 0.4! As a result, if you have outliers in your features (columns), normalizing your data will scale most of the data to a small interval. Standardisation is more robust to outliers.\n",
        "\n",
        "<div style=\" background-color:#3b3745;\n",
        "            text-align:left;\n",
        "            padding: 13px 13px;\n",
        "            border-radius: 8px;\n",
        "            margin: auto;\n",
        "            color: white\">\n",
        "<ul>\n",
        "MinMax Scaler shrinks the data within the given range which will affect the ability of the algorithm to give adequate weights for the features. The result looks valid - it looks like all our features have the same scale, but in reality the scale after transformation is not equal for them because the presence of outliers kills that idea!\n",
        "</div>\n",
        "\n",
        "Consider also that: according to scikit-learn documentation \"both StandardScaler and MinMaxScaler are very sensitive to the presence of outliers\".\n",
        "* [Compare the effect of different scalers on data with outliers](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html)\n",
        "\n",
        "but RobustScaler is much better:\n",
        "* [RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)"
      ],
      "metadata": {
        "id": "MYQl9fmOzfL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"background-color:#CDA63A;\n",
        "                                                 color:white;\n",
        "                                                 border-color:black;\n",
        "                                                 border-radius:5px;\n",
        "                                                 width:50%;\n",
        "                                                 margin: auto;\n",
        "                                                 text-align: left;\">\n",
        "It is important to remember that MinMAxScaler, StandardScaler and RobustScaler will not remove the outliers from a data set. We can say that standardization is more robust to outliers than normalization (especially using RobustScaler), because of the scaling process that we’ve just seen. In other words - the outliers will still remain in the data set after standardization and normalization.\n",
        "</div>"
      ],
      "metadata": {
        "id": "d6oJ3gJSzfL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (20,9))\n",
        "\n",
        "plt.subplot(2,3,1)\n",
        "ax = sns.histplot(df_ss[\"tenure\"], color='#008080', bins=72)\n",
        "ax.set(title = \"Standarization (StandardScaler)\")\n",
        "ax.set_xlabel('Tenure')\n",
        "\n",
        "plt.subplot(2,3,2)\n",
        "ax = sns.histplot(df_mm[\"tenure\"], color='#FF6347', bins=72)\n",
        "ax.set(title = \"Normalization (MinMaxScaler)\")\n",
        "ax.set_xlabel('Tenure')\n",
        "\n",
        "plt.subplot(2,3,3)\n",
        "ax = sns.histplot(df_rs[\"tenure\"], color='#FBDD7E', bins=72)\n",
        "ax.set(title = \"Standardization (RobustScaler)\")\n",
        "ax.set_xlabel('Tenure')\n",
        "\n",
        "plt.subplot(2,3,4)\n",
        "ax = sns.histplot(df_ss[\"TotalCharges\"], color='#008080', bins=72)\n",
        "ax.set_xlabel('Total Charges')\n",
        "\n",
        "plt.subplot(2,3,5)\n",
        "ax = sns.histplot(df_mm[\"TotalCharges\"], color='#FF6347', bins=72)\n",
        "ax.set_xlabel('Total Charges')\n",
        "\n",
        "plt.subplot(2,3,6)\n",
        "ax = sns.histplot(df_rs[\"TotalCharges\"], color='#FBDD7E', bins=72)\n",
        "ax.set_xlabel('Total Charges')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:50.100385Z",
          "iopub.execute_input": "2023-02-20T07:02:50.100746Z",
          "iopub.status.idle": "2023-02-20T07:02:52.017924Z",
          "shell.execute_reply.started": "2023-02-20T07:02:50.100713Z",
          "shell.execute_reply": "2023-02-20T07:02:52.016318Z"
        },
        "trusted": true,
        "id": "QUfPummQzfL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* After scaling features we can see that outcome is different nor MinMaxScaler and StandardScaler! 'TotalCharges' variable is highly skewed and it's represented in scale after standarization.\n",
        "* When we compare 'Tenure' with 'TotalCharges' we can see the difference in scale, but after normalization (MinMax scaler) the difference is gone - both variables are distributed between 0 and 1. This is important and not desirable!\n",
        "* Note in particular that because the values on each feature have different magnitudes, the spread of the transformed data on each feature is different also after standardization! It could be also problematic, but this is more real than forced transformation using MinMaxScaler.\n",
        "\n",
        "<div style=\" background-color:#3b3745;\n",
        "            text-align:left;\n",
        "            padding: 13px 13px;\n",
        "            border-radius: 8px;\n",
        "            margin: auto;\n",
        "            color: white\">\n",
        "<ul>\n",
        "In clustering analyses standardization may be especially crucial in order to compare similarities between features based on distance measures. Another example is the Principal Component Analysis (PCA), where we usually prefer standardization over Min-Max scaling because we are interested in the components that maximize the variance.\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "idGTQoCBzfL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the data is balanced we should consider deleting outliers.\n",
        "\n",
        "> Check this notebook for more information about dealing with outliers: [Outlier detection methods!](https://www.kaggle.com/code/marcinrutecki/outlier-detection-methods)"
      ],
      "metadata": {
        "id": "sAAWqD9wzfL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >🔝Back to Table of Contents🔝</a>"
      ],
      "metadata": {
        "id": "5h7Jm6tizfL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. One more example (and comparison)**"
      ],
      "metadata": {
        "id": "b6vnZea8zfL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a new data set: 60 random points from normal distribution with mean = 5 and standard deviation = 8"
      ],
      "metadata": {
        "id": "PB7UWxbdzfL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.random.normal(5, 8, 60)\n",
        "new_df = pd.DataFrame({\"Data\":data})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:52.01969Z",
          "iopub.execute_input": "2023-02-20T07:02:52.020182Z",
          "iopub.status.idle": "2023-02-20T07:02:52.027125Z",
          "shell.execute_reply.started": "2023-02-20T07:02:52.020139Z",
          "shell.execute_reply": "2023-02-20T07:02:52.025814Z"
        },
        "trusted": true,
        "id": "QNh90LF0zfL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.describe()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:52.028989Z",
          "iopub.execute_input": "2023-02-20T07:02:52.02933Z",
          "iopub.status.idle": "2023-02-20T07:02:52.056085Z",
          "shell.execute_reply.started": "2023-02-20T07:02:52.0293Z",
          "shell.execute_reply": "2023-02-20T07:02:52.054577Z"
        },
        "trusted": true,
        "id": "88DIITJYzfL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (20,6))\n",
        "\n",
        "ax = sns.histplot(new_df, color='#008080', kde=True, bins=60)\n",
        "ax.set(title = \"Dataframe without outliers\")\n",
        "ax.legend([],[], frameon=False)\n",
        "ax.axvline(new_df['Data'].median(), color='red', ls='--', lw=2)\n",
        "ax.axvline(new_df['Data'].mean(), color='blue', ls='--', lw=2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:52.057357Z",
          "iopub.execute_input": "2023-02-20T07:02:52.057683Z",
          "iopub.status.idle": "2023-02-20T07:02:52.562041Z",
          "shell.execute_reply.started": "2023-02-20T07:02:52.057653Z",
          "shell.execute_reply": "2023-02-20T07:02:52.560774Z"
        },
        "trusted": true,
        "id": "KK5nqbdDzfL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we add a few outliers to our input list."
      ],
      "metadata": {
        "id": "X10G2cFFzfL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outliers = np.random.uniform(200, 150, 5)\n",
        "out_df = pd.DataFrame({\"Data\": np.append(new_df, outliers)})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:52.563742Z",
          "iopub.execute_input": "2023-02-20T07:02:52.564375Z",
          "iopub.status.idle": "2023-02-20T07:02:52.571814Z",
          "shell.execute_reply.started": "2023-02-20T07:02:52.564338Z",
          "shell.execute_reply": "2023-02-20T07:02:52.570423Z"
        },
        "trusted": true,
        "id": "v6MheHVuzfL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_df.describe()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:52.573757Z",
          "iopub.execute_input": "2023-02-20T07:02:52.574187Z",
          "iopub.status.idle": "2023-02-20T07:02:52.597441Z",
          "shell.execute_reply.started": "2023-02-20T07:02:52.574121Z",
          "shell.execute_reply": "2023-02-20T07:02:52.596034Z"
        },
        "trusted": true,
        "id": "mZJ_afyFzfL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers caused the mean to increase significantly! Median, however, increased by a minimal amount."
      ],
      "metadata": {
        "id": "A9Z6rwapzfL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (20,6))\n",
        "\n",
        "ax = sns.histplot(out_df, color='#008080', kde=True, bins=60)\n",
        "ax.set(title = \"Dataframe with outliers\")\n",
        "ax.legend([],[], frameon=False)\n",
        "ax.axvline(out_df['Data'].median(), color='red', ls='--', lw=2)\n",
        "ax.axvline(out_df['Data'].mean(), color='blue', ls='--', lw=2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:52.599091Z",
          "iopub.execute_input": "2023-02-20T07:02:52.599828Z",
          "iopub.status.idle": "2023-02-20T07:02:53.012749Z",
          "shell.execute_reply.started": "2023-02-20T07:02:52.599781Z",
          "shell.execute_reply": "2023-02-20T07:02:53.011314Z"
        },
        "trusted": true,
        "id": "-Y0q6HorzfL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_names = out_df.columns\n",
        "\n",
        "df_robust = out_df.copy()\n",
        "df_stand = out_df.copy()\n",
        "df_norm = out_df.copy()\n",
        "\n",
        "df_robust = Robust_Scaler (df_robust, col_names)\n",
        "df_stand = Standard_Scaler (df_stand, col_names)\n",
        "df_norm = MinMax_Scaler (df_norm, col_names)\n",
        "\n",
        "df_robust2 = new_df.copy()\n",
        "df_stand2 = new_df.copy()\n",
        "df_norm2 = new_df.copy()\n",
        "\n",
        "df_robust2 = Robust_Scaler (df_robust2, col_names)\n",
        "df_stand2 = Standard_Scaler (df_stand2, col_names)\n",
        "df_norm2 = MinMax_Scaler (df_norm2, col_names)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:53.014091Z",
          "iopub.execute_input": "2023-02-20T07:02:53.014535Z",
          "iopub.status.idle": "2023-02-20T07:02:53.033289Z",
          "shell.execute_reply.started": "2023-02-20T07:02:53.014499Z",
          "shell.execute_reply": "2023-02-20T07:02:53.03191Z"
        },
        "trusted": true,
        "id": "V4_NKWEgzfL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (20,10))\n",
        "\n",
        "plt.subplot(2,3,1)\n",
        "ax = sns.histplot(df_stand2[\"Data\"], color='#008080', fill= True, alpha=.7, kde=True)\n",
        "ax.set(title = \"StandardScaler\")\n",
        "ax.set_xlabel(' ')\n",
        "\n",
        "plt.subplot(2,3,2)\n",
        "ax = sns.histplot(df_norm2[\"Data\"], color='#FF6347', fill= True, alpha=.7, kde=True)\n",
        "ax.set(title = \"MinMaxScaler\")\n",
        "ax.set_xlabel(' ')\n",
        "\n",
        "plt.subplot(2,3,3)\n",
        "ax = sns.histplot(df_robust2[\"Data\"], color='#FBDD7E', fill= True, alpha=.7, kde=True)\n",
        "ax.set(title = \"RobustScaler\")\n",
        "ax.set_xlabel(' ')\n",
        "\n",
        "plt.subplot(2,3,4)\n",
        "ax = sns.histplot(df_stand[\"Data\"], color='#008080', fill= True, alpha=.7, kde=True)\n",
        "ax.set(title = \"StandardScaler\")\n",
        "ax.set_xlabel(' ')\n",
        "\n",
        "plt.subplot(2,3,5)\n",
        "ax = sns.histplot(df_norm[\"Data\"], color='#FF6347', fill= True, alpha=.7, kde=True)\n",
        "ax.set(title = \"MinMaxScaler\")\n",
        "ax.set_xlabel(' ')\n",
        "\n",
        "plt.subplot(2,3,6)\n",
        "ax = sns.histplot(df_robust[\"Data\"], color='#FBDD7E', fill= True, alpha=.7, kde=True)\n",
        "ax.set(title = \"RobustScaler\")\n",
        "ax.set_xlabel(' ')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T07:02:53.034909Z",
          "iopub.execute_input": "2023-02-20T07:02:53.03526Z",
          "iopub.status.idle": "2023-02-20T07:02:54.393522Z",
          "shell.execute_reply.started": "2023-02-20T07:02:53.035229Z",
          "shell.execute_reply": "2023-02-20T07:02:54.392671Z"
        },
        "trusted": true,
        "id": "7zKfE5t-zfL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"background-color:#CDA63A;\n",
        "                                                 color:white;\n",
        "                                                 border-color:black;\n",
        "                                                 border-radius:5px;\n",
        "                                                 width:50%;\n",
        "                                                 margin: auto;\n",
        "                                                 text-align: left;\">\n",
        "RobustScaler produces a much wider range of values than the StandardScaler. That is expected. But wait a second - don't we expect the range of the transformed features to be also approximately similar?\n",
        "\n",
        "We can see that there is a significant difference when we compare scaled data with and without outliers. The difference is much lower with StandardScaler. So which scaler is the best in the end?\n",
        "</div>"
      ],
      "metadata": {
        "id": "R-q4nwLRzfL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\" background-color:#3b3745;\n",
        "            text-align:left;\n",
        "            padding: 13px 13px;\n",
        "            border-radius: 8px;\n",
        "            margin: auto;\n",
        "            color: white\">\n",
        "<ul>\n",
        "It is very important to highlight here that RobustScaler's results are not skewed by outliers and the spread represents REAL distance. It is more consistent with non-scaled data, even though it doesn't look like it at first glance.\n",
        "    \n",
        "* Remember that StandardScaler is also sensitive to outliers (less than MinMaxScaler, but still sensitive). The mean and standard deviation are highly affected by outliers. As a result the distance (measured in standard deviation) is also affected by outliers.\n",
        "\n",
        "* The spread of values after robust scaling is much higher, but there are only a few values with big distance (outliers) while the others are much closer. This is reality and the algorithm will be no mislead by this situation. On the contrary - it should work better.\n",
        "</div>"
      ],
      "metadata": {
        "id": "DNYNMS3LzfL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"6.1\"></a>\n",
        "## <b>6.1 <span style='color:#E1B12D'>Is it always better to use the RobustScaler?</span></b>"
      ],
      "metadata": {
        "id": "7r0MbvchzfL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When analyzing data, it's important to consider the amount of information that can be extracted from it. Robust measures like the interquartile range (IQR) are designed to focus on a specific subset of the data while ignoring potentially unreliable values. However, this deliberate exclusion of certain data can limit the overall amount of information that is used to make predictions, leading to lower confidence of the result.\n",
        "\n",
        "<div style=\" background-color:#3b3745;\n",
        "            text-align:left;\n",
        "            padding: 13px 13px;\n",
        "            border-radius: 8px;\n",
        "            margin: auto;\n",
        "            color: white\">\n",
        "<ul>\n",
        "Personally I would take advantage of StandardScaler if:\n",
        "\n",
        "* I decided to delete outliers or they are not present in a data set\n",
        "* When the data is not very highly skewed.\n",
        "\n",
        "I would use RobustScaler in opposite situations.\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "ZYPCMsNqzfL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >🔝Back to Table of Contents🔝</a>"
      ],
      "metadata": {
        "id": "-I8C9CEFzfL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"#toc\" role=\"button\" aria-pressed=\"true\" >🔝Back to Table of Contents🔝</a>"
      ],
      "metadata": {
        "id": "u-hbNa2QzfL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **References**"
      ],
      "metadata": {
        "id": "q8IJR-wczfL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://proclusacademy.com/blog/robust-scaler-outliers/\n",
        "\n",
        "https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html"
      ],
      "metadata": {
        "id": "dY9RfKk0zfL8"
      }
    }
  ]
}